#+SETUPFILE: ./export_template.org
#+TITLE: Machine Learning

* Reference Book                                                   :noexport:
[[file:~/Dropbox/Books/Programming/Machine%20Learning/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf][Pattern Recognition and Machine Learning]]
[[file:~/Dropbox/Books/Programming/Machine%20Learning/understanding-machine-learning-theory-algorithms.pdf][Understanding Machine Learning]]
* What is Learning?
An agent is said to be /learning/ if it improves its performance P on
task T based on experience/observations/data E. T must be fixed, P
must be measurable, E must exist. See [[file:artificial_intelligence.org::*Learning%20agents][Learning Agents]].
** Inductive Bias
The incorporation of prior knowledge biases the learning mechanism.
This is also called /inductive bias/. The incorporation of prior
knowledge is inevitable for the success of learning algorithms (the
no-free-lunch theorem). The stronger the prior knowledge that one
starts the learning process with, the easier it is to learn from
further examples.
* Types of Learning
** Active vs Passive Learning
An active learner interacts with the environment at training time,
(e.g. by posing queries or performing experiments), while a passive
learner only observes the information provided by the environment.
** Online vs Batch Learning
In online learning, the hypothesis has to be updated each time a new
label is received.
** Supervised vs Unsupervised Learning
In unsupervised learning, given a training set $S = \left(x_1, \hdots,
x_m\right)$, without a labeled output, one must construct a "good"
model/description of the data.

Example use cases include:
- clustering
- dimension reduction to ind essential parts of the data and reduce
  noise (e.g. PCA)
- minimises description length of data

** Reinforcement Learning
Each action in a staet has an associated cost and a probability
distribution of the next state.

Goal is to learn a policy (mapping from state to action) that
minimizes the sum of expected current and future costs.
** Supervised Learning
*** Measure of success
 A loss function helps measure our success. Given a set $H$ of
 hypothesis of models, and a domain $Z$, let $l$ be a function from $H
 \times Z$ to non-negative real numbers $l: $H \times Z \rightarrow
 \mathbb{R}_{+}$.

 The /risk function/ is the expected loss of the hypothesis,

 \begin{equation*}
   L_D(h) = E_{z \sim D}[l(h,z)]
 \end{equation*}

 We are interested in finding a hypothesis $h$ that has a small risk,
 or expected loss.
*** Empirical Risk Minimisation (ERM)
 - The training set error is often called the /empirical error/ or
   /empirical risk/.
 - Given a hypothesis class $H$, finding the hypothesis $h \in H$ that
   minimizes the empirical risk is a simple learning strategy.
*** Assumptions Made âš 
1. One common assumption is that the data in the data generation
   process is independently and identically distributed (IID),
   according to the distribution $D$.

Q: Given a large enough training set, do you expect the long term test
error to be similar to the training error?

- If IID, then yes
- If not, there is likely dependencies, but under certain conditions,
  yes.
  - If sampling mixes well, it will not take long for D' to look
    like a steady set distribution.
- If dependencies are exploited, there is a possibility of attaining
  lower training and test error.
* Concept Learning
A concept is a boolean-valued function over a set of input instances
(each comprising input attributes). Concept learning is a form of
supervised learning. Infer an unknown boolean-valued function from
training-examples.
** Hypothesis
There is a trade-off between /expressive power/ and smaller
/hypothesis space/. Large hypothesis spaces are bad, because search is
going to take a long time, and also requires more data. Humans exploit
structure in the hypothesis space to guide search and learn faster.

A hypothesis $h$ is consistent with a set of training examples $D$ iff
$h(x) = c(x)$ for all $<x,c(x)> \in D$.
** Inductive Learning
Any hypothesis found to approximate the target function well over a
sufficient large set of *training examples* will also approximate the
target function well over other *unobserved examples*.
** Concept Learning is Search
The goal is to search for a hypothesis $h \in H$ that is consistent
with $D$.
** Exploit Structure in Concept Learning
$h_j$ is more general than or equal to $h_k$ (denoted $h_j \ge_{g}
h_k$) iff any input instance $x$ that satisfies $h_j$ also satisfies
$h_k$.

This is relation is a *partial order*.

** Find-S Algorithm
Intuition: Start with the most specific hypothesis $h$. Whenever it
wrongly classifies a positive training example, we "minimally"
generalize it to satisfy its input instance.
*** Limitations
1. Can't tell whether Find-S has learnt the target concept
2. Can't tell when training examples are /inconsistent/
3. Picks a maximally specific $h$
4. Depending on $H$, there may be several solutions
** Version Space
\begin{equation*}
  VS_{H,D} = {h \in H | h \text{ is consistent with }D}
\end{equation*}
 
- If $c \in H$, then D can reduce $VS_{H,D}$ to ${c}$.
- If D is insufficient, then $VS_{H,D}$ represents the /uncertainty/
  of what the target concept is
- $VS_{H,D}$ contains all consistent hypotheses, including maximally
  specific hypotheses

The *general boundary* G of $VS_{H,D}$ is the set of maximally general
members of $H$ consistent with $D$.

The *specific boundary* S of $VS_{H,D}$ is the set of maximally general
members of $H$ consistent with $D$.

\begin{equation*}
  VS_{H,D} = {h \in H | \exists s \in S \exists g \in G g \ge_g h
    \ge_g s }
\end{equation*}

** List-Then-Eliminate Algorithm
Iterate through all hypotheses in $H$, and eliminate any hypothesis
found inconsistent with any training example. This algorithm is often
prohibitively expensive.

** Candidate-Elimination Algorithm
Start with most general and specific hypotheses. Each training example
"minimally" generalizes S and specializes G to remove inconsistent
hypotheses from version space.
* Density Estimation
/Density Estimation/ refers to the problem of modeling the probability
distribution $p(x)$ of a random variable $x$, given a finite set $x_1,
x_2, \hdots, x_n$ of observations.

We first look at parametric distributions, which are governed by a
small number of adaptive parameters. In a frequentist treatment, we
choose specific values for the parameters optimizing some criterion,
such as the likelihood function. In a Bayesian treatment, we
introduce prior distributions and then use Bayes' theorem to compute
the corresponding posterior distribution given the observed data.

An important role is played by /conjugate priors/, which yield
posterior distributions of the same functional form.

The maximum likelihood setting for parameters can give severely
over-fitted results for small data sets. To develop a Bayesian
treatment to this problem, we consider a form of prior distribution
with similar form as the maximum likelihood function. this property is
called /conjugacy/. For a binomial distribution, we can choose the
beta distribution as the prior.
* Refile
** Data Compression
In /lossy compression/, we seek to trade off code length with
reconstruction error.

In /vector quantization/, we seek a small set of vectors ${z_i}$ to
describe a large dataset of vectors ${x_i}$, such that we can
represent each $x-i$ with its closest approximation in ${z_i}$ with
small error. (Clustering problem)

In /transform coding/, we transform the data, usually using a linear
tranformation. The data in the transformed domain is quantized,
usually discarding the small coefficients, corresponding to removing
some of the dimensions.
