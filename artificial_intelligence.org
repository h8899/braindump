-*- mode: Org; org-download-image-dir: "./images/ai/"; -*-
#+SETUPFILE: ./export_template.org
#+TITLE: Artificial Intelligence
* Reference Books                                                  :noexport:
  [[file:~/Dropbox/Books/Programming/Artificial%20Intelligence/Stuart%20Russell,%20Peter%20Norvig-Artificial%20Intelligence.%20A%20Modern%20Approach%20%5BGlobal%20Edition%5D-Pearson%20(2016).pdf][AIMA]]
* What is Artificial Intelligence?
Designing agents that act rationally (e.g. through maximising a reward
function).

Humans often act in ways that do not maximise their own benefit
(irrational).
** Acting Humanly: Turing Test
A computer would require:

- natural language processing
- knowledge representation
- automated reasoning
- machine learning
** Thinking Humanly
Cognitive science brings together computer models and experimental
techniques in psychology to construct testable and provable theories
of the human mind.
** Thinking rationally
Taking informal knowledge and expressing it in logical terms.
** Acting Rationally
A rational agent is one that acts so as to achieve the best outcome
or,when there is uncertainty, the best expected outcome.

An agent is a function from percept histories to actions, i.e. $f: P^*
\rightarrow A$. We seek the best-performing agent for a certain task;
must consider computation limits.
* Intelligent Agents
- Agents perceive the environment through sensors
- Agents act upon the environment through actuators
** Rational Agents
- For each possible percept sequence, select an action that is
  expected to maximise its performance measure. The performance
  measure is a function of a given sequence of environment states.
- Given the evidence provided by the percept sequence and whatever
  built-in knowledge the agent has.
- Agents can perform actions that help them gather useful information
  (exploration)
- An agent is /autonomous/ if its behaviour is determined by its own
  experience (with ability to learn and adapt)
** Exploration vs Exploitation
Doing actions that modify future percepts (information gathering) is
an important part of rationality. In most scenarios, agents don't know
the entire environment /a priori/.
** Specifying Task Environment (PEAS)
- Performance measure
- Environment
- Actuators
- Sensors
** Properties of Task Environments
- Fully observable :: whether an agent's sensors gives it access to
     the complete state of the environment at any given point in time
- Deterministic ::  if the next state is completely determined by the
                   current environment. Otherwise it is *stochastic*.
- Episodic :: whether an agents experience is divided into atomic
              episodes. In each episode, an agent receives a percept
              and performs a single action. In *sequential*
              environments short-term actions can have long-term
              consequences. For this reason, episodic environments are
              generally simpler.
- Static :: whether the environment can change while the agent is
            deliberating.
- Discrete :: whether the state of the environment, how time is
              handled, and the percepts and actions of the agent
              discretely quantized.
- Single agent :: in some environments, for example chess, there are
                  multiple agents acting in the same environment.
  - cooperative :: if the two agents need to work together.
  - competitive :: if the two agents are working against each other.
- Known :: whether the agent knows the outcome of its actions.
** Table-Driven Agent
Simple to implement, and works. However, the number of table entries
is exponential in time: $\text{#percepts}^\text{time}$. Hence it is
doomed to failure. The key challenge to AI is to produce rational
behaviour from a small program rather than a vast table.
** Reflex agents
A simple reflex agent is one that selects actions on the basis of the
/current/ percept, ignoring the rest of the percept history. A
/condition-action/ rule is triggered upon processing the current
percept. E.g. *if* the car in front is braking, *then* brake too.

Basing actions on only the current percept can be highly limiting, and
can also lead to infinite loops. Randomized actions of the right kind
can help escape these infinite loops.
** Model-based Reflex Agents
The agent maintains some *internal state* that depends on percept
history and reflects at least some of the unobserved aspects of the
current state. Information about how the world evolves independently
from the agent is encoded into the agent. This knowledge is called a
*model* of the world, and this agent is hence a *model-based* agent.
** Goal-based agents
Knowing about the current state of the environment may not be enough
to decide on what to do. Agents may need *goal* information that
describes situations that are desirable. Sometimes goal-based action
selection is straightforward, but in others *searching* and *planning*
are required to achieve the goal. Goal-based agents are flexible
because the knowledge that supports its decisions is represented
explicitly and can be modified, although it is less efficient.
** Utility-based agents
Goals provide a binary distinction between good and bad states. A more
general performance measure should allow a comparison between world
states according to exactly how good it is to the agent. An agent's
*utility function* is an internalisation of the performance measure.
An agent chooses actions to maximise its expected utility. A
utility-based agents has to model and keep track of its environment.
** Learning agents
A learning agent can be divided into four conceptual components.
- learning element :: responsible for making improvements
- performance element :: responsible for selecting extrenal actions
- problem generator :: suggests actions that will lead to new and
     informative experiences

the learning element takes in feedback from the *critic* on how the
agent is doing and determines show the performance element should be
modified to do better in the future.
** State representations
*** Atomic Representation
In an atomic representation each state of the world is indivisible,
and has no internal structure. Search, game-playing, hidden Markov
models and Markov decision processes all work with atomic
representations.
*** Factored Representation
A factored representation splits up each state into a fixed set of
*variables* or *attributes*, each of which can have a *value*.

Constraint satisfaction algorithms, propositional logic, planning,
Bayesian networks and machine learning algorithms work with factored
representations.

*** Structured Representations
Structured representations underlie relational databases and
first-order logic, first-order probability models, knowledge-based
learning and much of natural language understanding.
*** Implications
A more expressive representation can capture, at least as concisely, a
everything a more expressive one can capture, plus more. On the other
hand, reasoning and learning become more complex as the expressive
power of the representation increases.
* Problem-Solving
Problem-solving agents use /atomic/ representations, as compared to
goal-based agents, which use more advanced factored or structured
representations.

The process of looking for a sequence of actions that reaches the goal
is called /search/. A search algorithm takes a problem as input and
returns a /solution/ in the form of an action sequence.
** Searching for Solutions
*** How Search Algorithms Work
 Search algorithms consider various possible action sequences. The
 possible action sequences start at the initial state form a /search
 tree/.

 Search algorithms require a data structure to keep track of the search
 tree that is being constructed.

 - state :: state in the state space to which the node corresponds
 - parent :: the node in the search tree that generated this node
 - action :: the action that was applied to the parent to generate this node
 - path-cost :: the cost, traditionally denoted by $g(n)$, of the path
                from the initial state to the node, as indicated by the
                parent pointers
*** Measuring Performance
 - completeness :: is the algorithm guaranteed to find a solution if
                   it exists?
 - optimality :: does the strategy find the optimal solution?
 - time complexity :: how long does it take to find a solution?
 - space complexity :: how much memory is required to do the search?
*** Uninformed Search Strategies
**** Breadth-first Search
The root node is expanded first, then all the successors of the root
node are expanded next, then their successors, and so on.

| performance      | rating   |
|------------------+----------|
| completeness     | YES      |
| optimal          | NO       |
| time complexity  | $O(b^d)$ |
| space complexity | $O(b^d)$ |

The shallowest node may not be the most optimal node.

The space used in the /explored set/ is $O(b^{d-1})$ and the space
used in the /frontier/ is $O(b^d)$.

In general, exponential-complexity search problems cannot be solved by
uninformed methods for any but the smallest instances.
**** Uniform-cost Search
Uniform-cost search expands the node $n$ with the lowest path
cost $g(n)$. The goal test is applied to a node when it is selected
for expansion rather than when it is first generated.

This is equivalent to BFS if all step costs are qual.

| performance  | rating                                                                            |
|--------------+-----------------------------------------------------------------------------------|
| completeness | MAYBE                                                                             |
| optimal      | YES                                                                               |
| time         | $O(b^{1+\lfloor{\frac{C^*}{\epsilon}}\rfloor})$, where $C^*$ is the optimal cost. |
| space        | $O(b^{1+\lfloor{\frac{C^*}{\epsilon}}\rfloor})$                                   |

Completeness is guaranteed only if the cost of every step exceeds some
small positive constant $\epsilon$. an infinite loop may occur if
there is a path with an infinite sequence of zero-cost actions.
**** Depth-first Search
Always expands the /deepest/ node in the current frontier of the
search tree.

| performance      | rating                        |
|------------------+-------------------------------|
| completeness     | YES                           |
| optimal          | NO                            |
| time complexity  | $O(b^m)$                      |
| space complexity | $O(b^m)$, $O(m)$ if backtrack |

The time complexity of DFS may be worse than BFS: $O(b^m)$ might be
larger than $O(b^d)$.

DFS only requires storage of $O(bm)$ nodes, where $m$ is the maximum
depth of any node. *backtracking search* only generates one successor
at a time, modifying the current state description rather than copying
it. Memory requirements reduce to one state description and $O(m)$
actions.
**** Depth-limited Search
In depth-limited search, nodes at depth of pre-determined limit $l$
are treated as if they had no successors. This limit solves the
infinite-path problem.

| performance      | rating                        |
|------------------+-------------------------------|
| completeness     | YES                           |
| optimal          | NO                            |
| time complexity  | $O(b^l)$                      |
| space complexity | $O(b^l)$, $O(l)$ if backtrack |
**** Iterative Deepening Depth-first Search
Key idea is to gradually increase the depth limit: first 0, then
1, then 2... until a goal is found.


[[file:images/ai/Problem-Solving/screenshot_2018-01-22_15-26-50.png]]


$N(IDS) = (d)b + (d-1)b^2 + \hdots + (1)b^d$, which gives a time
complexity of $O(b^d)$

| performance      | rating                        |
|------------------+-------------------------------|
| completeness     | YES                           |
| optimal          | NO (unless step cost is 1)    |
| time complexity  | $O(b^d)$                      |
| space complexity | $O(b^d)$, $O(m)$ if backtrack |


1. BFS and IDS are complete if $b$ is finite.
2. UCS is complete if $b$ is finite and step cost is $\ge \epsilon$.
3. BFS and IDS are optimal if all step costs are identical.
**** Bidirectional Search
Conduct two simultaneous searches -- one forward from the initial
state, and the other backward from the goal. This is implemented by
replacing the goal test with a check to see whether the frontiers of
two searches intersect. This reduces the time ad space complexity to $O(b^{d/2})$.
