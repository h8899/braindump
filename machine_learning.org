#+SETUPFILE: ./export_template.org
#+TITLE: Machine Learning

* Reference Book                                                   :noexport:
[[file:~/Dropbox/Books/Programming/Machine%20Learning/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf][Pattern Recognition and Machine Learning]]
[[file:~/Dropbox/Books/Programming/Machine%20Learning/understanding-machine-learning-theory-algorithms.pdf][Understanding Machine Learning]]
* What is Learning?
An agent is said to be /learning/ if it improves its performance P on
task T based on experience/observations/data E. T must be fixed, P
must be measurable, E must exist. [[file:artificial_intelligence.org::*Learning%20agents][Learning Agents]]
** Batch Learning
** Online Learning
The hypothesis has to be updated each time a new label is received.
** Unsupervised Learning
Given a training set $S = \left(x_1, \hdots, x_m\right)$, without a
labeled output, construct a "good" model/description of the data.

- clustering
- dimension reduction to ind essential parts of the data and reduce
  noise (e.g. PCA)
- minimises description length of data
*** Data Compression
In /lossy compression/, we seek to trade off code length with
reconstruction error.

In /vector quantization/, we seek a small set of vectors ${z_i}$ to
describe a large dataset of vectors ${x_i}$, such that we can
represent each $x-i$ with its closest approximation in ${z_i}$ with
small error. (Clustering problem)

In /transform coding/, we transform the data, usually using a linear
tranformation. The data in the transformed domain is quantized,
usually discarding the small coefficients, corresponding to removing
some of the dimensions.
** Reinforcement Learning
Each action in a staet has an associated cost and a probability
distribution of the next state.

Goal is to learn a policy (mapping from state to action) that
minimizes the sum of expected current and future costs.
** Supervised Learning
*** Measure of success
 A loss function helps measure our success. Given a set $H$ of
 hypothesis of models, and a domain $Z$, let $l$ be a function from $H
 \times Z$ to non-negative real numbers $l: $H \times Z \rightarrow
 \mathbb{R}_{+}$.

 The /risk function/ is the expected loss of the hypothesis,

 \begin{equation*}
   L_D(h) = E_{z \sim D}[l(h,z)]
 \end{equation*}

 We are interested in finding a hypothesis $h$ that has a small risk,
 or expected loss.
*** Empirical Risk Minimisation (ERM)
 - The training set error is often called the /empirical error/ or
   /empirical risk/.
 - Given a hypothesis class $H$, finding the hypothesis $h \in H$ that
   minimizes the empirical risk is a simple learning strategy.
*** Assumptions Made âš 
1. One common assumption is that the data in the data generation
   process is independently and identically distributed (IID),
   according to the distribution $D$.

Q: Given a large enough training set, do you expect the long term test
error to be similar to the training error?

- If IID, then yes
- If not, there is likely dependencies, but under certain conditions,
  yes.
  - If sampling mixes well, it will not take long for D' to look
    like a steady set distribution.
- If dependencies are exploited, there is a possibility of attaining
  lower training and test error.

* Concept Learning
A concept is a boolean-valued function over a set of input instances
(each comprising input attributes). Concept learning is a form of
supervised learning. Infer an unknown boolean-valued function from
training-examples.
** Hypothesis
There is a trade-off between /expressive power/ and smaller
/hypothesis space/. Large hypothesis spaces are bad, because search is
going to take a long time, and also requires more data. Humans exploit
structure in the hypothesis space to guide search and learn faster.

A hypothesis $h$ is consistent with a set of training examples $D$ iff
$h(x) = c(x)$ for all $<x,c(x)> \in D$.
** Inductive Learning
Any hypothesis found to approximate the target function well over a
sufficient large set of *training examples* will also approximate the
target function well over other *unobserved examples*.
** Concept Learning is Search
The goal is to search for a hypothesis $h \in H$ that is consistent
with $D$.
** Exploit Structure in Concept Learning
$h_j$ is more general than or equal to $h_k$ (denoted $h_j \ge_{g}
h_k$) iff any input instance $x$ that satisfies $h_j$ also satisfies
$h_k$.

This is relation is a *partial order*.
