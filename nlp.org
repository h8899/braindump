#+SETUPFILE: ./export_template.org
#+TITLE: Natural Language Processing
* How to Represent Words
http://ruder.io/word-embeddings-2017/index.html

How do we represent words as input to any of our models? There are an
estimated *13 million* tokens for the English language, some of them
have similar meanings.

The simplest representation is *one-hot vector*: we can represent
every word as a $\mathbb{R}^{|V|x1}$ vector with all 0s and one 1 at
the index of that word in the sorted english language.

To reduce the size of this space, we can use SVD-based methods. For
instance, accumulating co-occurrence count into a matrix =X=, and
perform SVD on =X= to get a $USV^T$ decomposition. We can then use the
rows of =U= as the word embeddings for all words in the dictionary.
** Window-based Co-occurence Matrix
Count the number of times each word appears inside a window of a
particular size around the word of interest. We calculate this count
for all words in the corpus.
** Issues with SVD-based methods
1. Dimensions of the matrix can change very often (new words added to corpus)
2. Matrix is extremely sparse as most words don't co-occur
3. Matrix is very high-dimensional
4. Quadratic cost to perform SVD

Some solutions include:
1. Ignore function words (blacklist)
2. Apply a ramp window (distance between words)
3. Use [[https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide.php][Pearson correlation]] and set negative counts to 0 instead of raw count.
** Iteration-based Methods (e.g. word2vec)
Design a model whose parameters are the word vectors, and train the
model on a certain objective. Discard the model, and use the vectors
learnt.
Efficient tree structure to compute probabiltiies for all the vocabulary
*** Language Models
  The model will need to assign a probability to a sequence of tokens.
  We want a high probability for a sentence that makes sense, and a low
  probability for a sentence that doesn't.

  Unigrams completely ignore this, and a nonsensical sentence might
  actually be assigned a high probability. Bigrams are a naive way of
  evaluating a whole sentence.
**** Continuous bag-of-words (CBOW)
  Predicts a center word from the surrounding context in terms of word
  vectors.

  For each word, we want to learn 2 vectors:
  1. v: (input vector) when the word is in the context
  2. u: (output vector) when the word is in the center


  - known parameters
    - input :: sentence represented by one-hot vectors
    - output :: one hot vector of the known center word

  Create 2 matrices $\mathbb{V} \in \mathbb{R}^{n \times |V|}$ and
  $\mathbb{U} \in \mathbb{R}^{|V| \times n}$, where n is the arbitrary
  size which defines the size of the embedding space.

  $V$ is the input word matrix such that the /i/-th column of $V$ is the
  $n$-dimensional embedded vector for word $w_i$ when it is an input to
  this model. $U$ is the output word matrix.

  1. Generate one-hot word vectors for the input context of size m.
  2. Get our embedded word vectors for the context: ($v_{c-m} =
     Vx^{c-m}$, ...)
  3. Average these vectors to get $\hat{v} = \frac{\sum v}{2m} \in \mathbb{R^n}$
  4. Generate a score vector $z = U\hat{v} \in \mathbb{R}^{|V|}$ As the
     dot product of similar vectors is higher, it will push simila words
     close to each other in order to achieve a high score.
  5. Turn the scores into probabilities $\hat{y} = softmax(z) \in \mathbb{R}^{|V|}$

  Minimise loss function (cross entropy), and use stochastic gradient
  descent to update all relevant word vectors.
**** TODO Skipgram
http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/

Predicts the distribution of context words from a center word. This is
very similar to the CBOW approach, wind the input and output vectors
reversed. Here, a naive Bayes assumption is invoked: that given the
center word, all output words are completely independent.

Input vector: one-hot vectors corresponding to the vocabulary

The neural network is consists of one hidden layer of $n$ units, where
$n$ is the desired dimension of the word vector. The output layer is a
softmax regression layer, outputting a value between 0 and 1. We want
the value for the context words to be high, and the non-context words
to be low.

After training, the output layer is discarded, and the weights at the
hidden layer are the word vectors we want.

*** Training Methods
In practice, negative sampling works better for frequently occurring
words and lower-dimensional vectors, and hierachical softmax works
better for infrequent words.
**** TODO Negative Sampling
Take $k$ negative samples, and minimise the probability of the two
words co-occurring while also maximising the probability of the two
words in the same window co-occur.
**** TODO Hierachical Softmax
Hierachical Softmax uses a binary tree to represent all words in the
vocabulary. Each leaf of the tree is a word, and there is a unique
path from root to leaf. The probability of a word $w$ given a vector
$w_i$, $P(w|w_i)$, is equal to the probability of a random walk
starting from the root and ending in the leaf node corresponding to
$w$.



** Global Vectors for Word Representation (GloVe)
We look at how word vectors can be evaluated intrinsically and
extrinsically. Count-based methods of generating word embeddings rely
on global statistical information, and do poorly on tasks such as word
analogy, indicating a sub-optimal vector space structure.

word2vec presents a window-based method of generating word-embeddings
by making predictions in context-based windows, demonstrating the
capacity to capture complex linguistic patterns beyond word
similarity.

GloVe consists of a weighted least squares model that trains on global
word-word co-occurrence counts, and produces a word vector space with
meaningful sub-structure. It shows state-of-the-art performance on the
word analogy task.


*** Co-occurrence Matrix
Let $X$ denote the word-word co-occurrence matrix, where $X_{ij}$
indicate the number of times word $j$ occur in the context of word
$i$. Let $X_i = \sum_k X_{ik}$ be the number of times any word $k$
appears in the context of word $i$. Let $P_{ij} = P(w_j|w_i) =
\frac{X_{ij}}{X_i}$ be the probability of j appearing in the context
of word $i$.

* Word Vector Evaluation

** Intrinsic
- Evaluation on a specific/intermediate subtask
- Fast to compute
- Helps understand the system
- Not clear if really helpful unless correlation to real task is established

** Extrinsic
- Evaluation on a real task
- Can take a long time to compute accuracy
- Unclear if the subsystem is the problem or its interaction or other subsystems
- If replacing exactly one subsystem with another improves accuracy âž¡ Winning!
