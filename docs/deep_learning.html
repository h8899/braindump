<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-01-13 Sat 11:58 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Deep Learning</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Jethro Kuan" />
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Deep Learning</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org7a95a75">1. Bayesian probability</a></li>
<li><a href="#org0b4594f">2. Universal Approximation Theorem</a></li>
<li><a href="#orgeec8f39">3. Introduction</a></li>
<li><a href="#org030cfff">4. Measure Theory</a>
<ul>
<li><a href="#org50c5e91">4.1. Kullback-Leiber (KL) divergence</a></li>
<li><a href="#org1b13c79">4.2. Cross Entropy</a></li>
</ul>
</li>
<li><a href="#orgb36df10">5. SVD</a></li>
<li><a href="#orgad59154">6. Learning Rates</a>
<ul>
<li><a href="#orgf19edff">6.1. Learning Rate Annealing</a></li>
</ul>
</li>
<li><a href="#org3b54b83">7. Reinforcement Learning</a></li>
<li><a href="#org7a89d3f">8. How Companies use Deep Learning</a>
<ul>
<li><a href="#org779c193">8.1. ViSenze</a>
<ul>
<li><a href="#orgdf09d8f">8.1.1. Things they focused on as a company</a></li>
<li><a href="#orgd09cdea">8.1.2. Visual Embeddings Used</a></li>
<li><a href="#org0d54a8c">8.1.3. Lessons Learnt</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org7a95a75" class="outline-2">
<h2 id="org7a95a75"><span class="section-number-2">1</span> Bayesian probability</h2>
<div class="outline-text-2" id="text-1">
<p>
The quantification of the expression of uncertainty. We can make
precise revisions of uncertainty in light of new evidence.
</p>

<p>
In the example of polynomial curve fitting, it seems reasonable to
apply the frequentist notion of probability to the random values of
the observed variables. However, we can use probability theory to
describe the uncertainty in model parameters, and the choice of model
itself.
</p>

<p>
We can evaluate the uncertainty in model parameters <code>w</code> after we have
observed some data <code>D</code>. \(p(D|w)\) is called the <i>likelihood function</i>.
</p>

<p>
The posterior probability is proportional to <code>likelihood x prior</code>.
Model parameters <code>w</code> are chosen to maximise the likelihood function
\(p(D|w)\). Because the negative log-likelihood is a monotonically
decreasing function, minimising it is equivalent to maximising
likelihood.
</p>
</div>
</div>
<div id="outline-container-org0b4594f" class="outline-2">
<h2 id="org0b4594f"><span class="section-number-2">2</span> Universal Approximation Theorem</h2>
<div class="outline-text-2" id="text-2">
<p>
<a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">https://en.wikipedia.org/wiki/Universal_approximation_theorem</a>
A feed-forward network with a single hidden layer containing a finite
number of neurons can approximate continuous functions on compact
subsets of the real space.
</p>
</div>
</div>
<div id="outline-container-orgeec8f39" class="outline-2">
<h2 id="orgeec8f39"><span class="section-number-2">3</span> Introduction</h2>
<div class="outline-text-2" id="text-3">
<p>
Many of the factors of variation inï¬‚uence every single piece of data
we are able to observe.
</p>

<p>
Deep learning solves this central problem in representation learning
by introducing representations that are expressed in terms of other,
simpler representations. Deep learning enables the computer to build
complex concepts out of simpler concepts.
</p>
</div>
</div>

<div id="outline-container-org030cfff" class="outline-2">
<h2 id="org030cfff"><span class="section-number-2">4</span> Measure Theory</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org50c5e91" class="outline-3">
<h3 id="org50c5e91"><span class="section-number-3">4.1</span> Kullback-Leiber (KL) divergence</h3>
<div class="outline-text-3" id="text-4-1">
<p>
The KL divergence is 0 iff P and Q are the same distribution in
discrete variables, and equal almost everywhere in continuous
variables. Because the KL divergence is non-negative and measures the
difference between the two distributions, it is soften conceptualised
as some sort of distance, but it is not a true measure, because it is
not symmetric.
</p>
</div>
</div>
<div id="outline-container-org1b13c79" class="outline-3">
<h3 id="org1b13c79"><span class="section-number-3">4.2</span> Cross Entropy</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Minimising the cross entropy with respect to Q is equivalent to
minimising the KL divergence.
</p>
</div>
</div>
</div>
<div id="outline-container-orgb36df10" class="outline-2">
<h2 id="orgb36df10"><span class="section-number-2">5</span> SVD</h2>
<div class="outline-text-2" id="text-5">
<p>
<a href="https://www.youtube.com/watch?v=P5mlg91as1c">https://www.youtube.com/watch?v=P5mlg91as1c</a>
</p>

<p>
<a href="https://www.quora.com/For-a-non-expert-what-is-the-difference-between-Bayesian-and-frequentist-approaches/answer/Jason-Eisner">https://www.quora.com/For-a-non-expert-what-is-the-difference-between-Bayesian-and-frequentist-approaches/answer/Jason-Eisner</a>
</p>
</div>
</div>
<div id="outline-container-orgad59154" class="outline-2">
<h2 id="orgad59154"><span class="section-number-2">6</span> Learning Rates</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-orgf19edff" class="outline-3">
<h3 id="orgf19edff"><span class="section-number-3">6.1</span> Learning Rate Annealing</h3>
<div class="outline-text-3" id="text-6-1">
<p>
Decay learning rate after several iterations&#x2026;
</p>

<p>
Also: SGDR with cyclic learning rate, restores high learning rate
after several iterations to try to find local minima that is largely
flat, and doesn't change so much in any direction. (Generalises
better)
</p>
</div>
</div>
</div>
<div id="outline-container-org3b54b83" class="outline-2">
<h2 id="org3b54b83"><span class="section-number-2">7</span> Reinforcement Learning</h2>
<div class="outline-text-2" id="text-7">
<p>
<a href="https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287">https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287</a>
</p>
</div>
</div>
<div id="outline-container-org7a89d3f" class="outline-2">
<h2 id="org7a89d3f"><span class="section-number-2">8</span> How Companies use Deep Learning</h2>
<div class="outline-text-2" id="text-8">
</div>
<div id="outline-container-org779c193" class="outline-3">
<h3 id="org779c193"><span class="section-number-3">8.1</span> ViSenze</h3>
<div class="outline-text-3" id="text-8-1">
<p>
Visenze's primary product is their Visual Search (reverse image
search).
</p>

<p>
Initially, they started out with a similar model to our approach:
Train a CNN, read encodings before the FC layer, use it to perform NN
search.
</p>

<p>
Now their pipeline is as follows:
Query time -&gt; object detection -&gt; Extract Features -&gt; Nearest Neighbours -&gt; Ranked Results
Offline training -&gt; Detection Model -&gt; Embedding models -&gt; Nearest Neighbours
Index time -&gt; Objects -&gt; Extract Features -&gt; Search Index
(Compression/Hash Model)
</p>

<p>
Object detection followed the trends in research papers:
</p>
<ol class="org-ol">
<li>R-CNN</li>
<li>Fast-CNN</li>
<li>Faster-CNN</li>
<li>YOLO/SSD</li>
<li>DSOD (Current)</li>
</ol>

<p>
Model performance is based of standard IR metrics: They are using DCG
score for evaluating their reverse image search. This requires a lot
of manual annotation.
</p>

<p>
Models are trained offline using physical purchased GPUs.
</p>

<p>
Deployment: Kubernetes on AWS, with generally small CPU servers.
Overall latency is less than 200ms.
</p>
</div>

<div id="outline-container-orgdf09d8f" class="outline-4">
<h4 id="orgdf09d8f"><span class="section-number-4">8.1.1</span> Things they focused on as a company</h4>
<div class="outline-text-4" id="text-8-1-1">
<ol class="org-ol">
<li>Tooling:
<ol class="org-ol">
<li>Annotation System
<ol class="org-ol">
<li>Complete annotation is crucial to detection training</li>
</ol></li>
<li>Training System
<ol class="org-ol">
<li>Make it easy to change hyperparameters and retrain models</li>
<li>Abstract away need for knowing deep learning</li>
<li>Platform for tracking metrics</li>
</ol></li>
<li>Querylog pipeline
<ol class="org-ol">
<li>Take user input as training data</li>
</ol></li>
<li>Evaluation System
<ol class="org-ol">
<li>Both automatic evaluation via metrics and manual (AB testing)
is done before release</li>
<li>Visualisations via T-SNE to see if clusters remain the
same/make sense, when new learnings are added: <b>learning
without forgetting</b></li>
</ol></li>
</ol></li>
<li>Business:
<ol class="org-ol">
<li>Attend to customer requirements: e.g. if a company wants to sell
hats, model needs to be trained to detect hats, and these
learnings need to be added to the existing model without
affecting data earlier</li>
</ol></li>
</ol>
</div>
</div>
<div id="outline-container-orgd09cdea" class="outline-4">
<h4 id="orgd09cdea"><span class="section-number-4">8.1.2</span> Visual Embeddings Used</h4>
<div class="outline-text-4" id="text-8-1-2">
<p>
At Visenze, they use multiple embeddings in different feature spaces
to measure similarity. The results are then combined before returned.
The 4 main embeddings are:
</p>

<ol class="org-ol">
<li>Exact matches (same item)
<ol class="org-ol">
<li>Trained with siamese network with batched triplet loss
(typically used in face recognition, but seems to work well with
product classification)</li>
</ol></li>
<li>Same Category
<ol class="org-ol">
<li>Domain specific labels have been most helpful in achieving
state-of-the-art accuracy
<ol class="org-ol">
<li>e.g for fashion, sleeve length, jeans length etc.</li>
</ol></li>
</ol></li>
<li>Similar Categories</li>
</ol>
</div>
</div>
<div id="outline-container-org0d54a8c" class="outline-4">
<h4 id="org0d54a8c"><span class="section-number-4">8.1.3</span> Lessons Learnt</h4>
<div class="outline-text-4" id="text-8-1-3">
<ol class="org-ol">
<li>Taxonomy Coverage</li>
<li>Training Data Coverage
<ol class="org-ol">
<li>Obtaining training data from one source only can lead to severe
bias (e.g. detecting watermarks and using it as feature)</li>
</ol></li>
<li>Overfitting</li>
<li>Continuous Improvement (Learning Without Forgetting)</li>
<li>Bad-case driven development
<ol class="org-ol">
<li>Be quick to identify hard negatives, and add in similar
negative samples into training data to improve accuracy</li>
</ol></li>
<li>Image quality, rotation</li>
<li>Re-ranking based on customer requirements</li>
</ol>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Jethro Kuan</p>
<p class="date">Created: 2018-01-13 Sat 11:58</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
