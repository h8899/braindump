* Operating System Key Concepts
** Motivation
1. Abstraction
   - Presents common high level functionality to users
   - Efficient and Portable
2. Resource Allocator
4. Program Control
   - Monitor execution of program, and manage resource access privileges
** Alarm signal
 The alarm signal causes the operating system to suspend whatever it is
 doing, save its registers on the stack, and start running a special
 signal-handling procedure.
** Identification
 Each person authorized to use the OS is assigned a UID (User
 Identification). Each process started has the UID of the person who
 started it. The child process inherits the UID from the parent. Users
 can also be members of groups, each with a GID (Group Identification).

** File systems
 Before a file can be read or written, it must be opened, at which time
 the permissions are checked. If access is permitted, the system
 returns a small integer called the *file descriptor* to use in
 subsequent operations.

 *Special files* are provided in order tomake I/O devices look like
 files. That way, they can be read and written using the same system
 calls as are used for reading and writing files. *block special files*
 are used to model devices that consist of a collection of randomly
 addressable blocks, such as disks. A program can open a block special
 file, and access a particular block to read it. *character special
 files* are used to model printers, modems and other devices that
 accept or output a character stream.

** Pipe
 A sort of pseudofile that can be used to connect two processes. If
 process A and B wish to talk using a pipe, they must set it up in
 advance.

 [[file:./images/screenshot-04.png]]

** Process segments
 [[file:./images/screenshot-05.png]]

** Files in UNIX
 Every file in UNIX has a unique number, its i-number, that identifies
 it. The i-number is an index into a table of *i-nodes*, one per file,
 telling who owns the file, where its disk-blocks are and so one. A
 directory is a file containing a set of (i-number, ASCII name) pairs.

** Hypervisors
 In practice, the real distinction between a type 1 hypervisor and a
 type 2 hypervisor is that a type 2 makes uses of a host operating
 system and its file system to create processes, store files, and so
 on. A type 1 hypervisor has no underlying sup- port and must perform
 all these functions itself.

* Process
A program in execution. Associated with each process is a *address
space*. Address space is a list of memory locations from 0 to maximum
in which the program can write to.

Information about all the processes are stored in the operating system
table called the *process table*, which is an array structure, one for
each process currently in existence.

A suspended process consists of its address space, and its entry in
the process table, which contains the contents of the registers and
other items required to resume the process later on.
** Process Creation
OS needs to load its code and any static data (e.g. initialized
variables) into memory, into the address space of the process. Modern
OSes load these lazily, via the machinery of paging and swapping.

1. Run-time stack
   - Used for local variables, function parameters and return
     addresses
2. Heap
   - Dynamically allocated data, programs request this space by
     calling =malloc()= and free it explicitly using =free()=
3. I/O related tasks
   - 3 open file descriptors: =stdin=, =stdout= and =stderr=
** Function Call
*** Control Flow issues
1. Need to jump to the function body
2. Need to resume when the function call is done
3. Minimally, need to store the PC of the caller
*** Data Storage issues
1. Need to pass parameters to the function
2. Need to capture the return result
3. May have local variable declarations
*** Stack Memory
Define new region of memory, called *stack memory*, for function
invocations. A new hardware register, the *stack pointer*, stores
the current memory address of the top of the stack.

When the stack grows, the stack pointer decreases. The stack grows
from bottom up. This is true for most architectures.
*** Stack Frame
The stack memory stores a bunch of stack frames, one stack frame for
each function invocation. The stack frame stores:

1. Local variables
2. Parameters
3. Return PC
4. Saved Registers
5. Saved Stack Pointer
6. Frame Pointer
*** Function Call Convention (FCC)
There are different ways to setup stack frames. An example scheme is
described below.

1. Caller passes parameters with registers and/or stack
2. Caller saves return PC on stack
3. *Transfer Control from Caller to Callee*
4. Callee save registers used by callee. Save old SP and FP
6. Callee allocates space for local variables on stack
7. Callee updates stack pointer to top of stack

Teardown:

1. Callee: Restore saved registers, FP, SP
2. *Transfer control from callee to caller using saved PC*
3. Caller: Continues execution in caller

*** Frame Pointer
Stack Pointer is hard to use as it can change. Frame pointer points to
a fixed location in a stack frame, and other items are accessed as
offsets from the frame pointer.
** Dynamic Memory Allocation
High Level Languages allow dynamic allocation of memory space, e.g.
C's =malloc=. These memory blocks have different behaviours. First,
they are only allocated at runtime, and hence cannot be placed in the
data region. Next, there is no definition deallocation timing, and
hence cannot be placed in the stack region.

Hence, a new region is needed, called the heap. Heap memory is a lot
trickier to manage. Variable size, and allocation/deallocation timing
is not known before hand.
** Process Identification
To distinguish processes from each other, a process ID (PID) is
assigned to each process.
** 5-state Process Model
   - New :: The process creation is started, but has not been
            allocated the required resources.
   - Ready :: Process is ready to run, but the OS has not chosen to run
              it yet.
   - Running :: A process is running if it is executing instructions on
                the processor.
   - Blocked :: Process has performed some kind of operation that
                makes it not ready to run until another event has
                taken place, e.g. being blocked by I/O.
   - Terminated :: Process is finished, may require OS cleanup.

[[file:./images/screenshot-02.png]]

** Data structures required
   - Process Table :: keeps track of all processes
   - PCB :: contains the entire execution context for a process

[[file:./images/screenshot-03.png]]

** Mechanism: Limited Direct Execution
The OS must virtualize the CPU in an efficient manner, while retaining
control over the system. To do so, both hardware and operating systems
support will be required. The OS will often use a judicious bit of
hardware support in order to accomplish its work effectively.
** Access Control
In /user mode/, applications do not have full access to hardware
resources. The OS runs in /kernel mode/, which has access to the full
resources of the machine.

Code can request access to system resource by calling the /trap/ call,
which raises the privilege level to kernel mode. Once finished, the OS
calls the /return-from-trap/ instruction, which returns the calling
user program, while reducing the privilege level back to user mode.

During bootup, the machine is started in kernel mode. The OS sets up a
trap table, and informs the hardware of the location of specialised
/trap handlers/, which is the code to run when certain exceptional
events occur. One such example is the hard-disk interrupt.
** General System Call Mechanism
   1. User program invokes the library call, using the normal function
      call mechanism
   2. Library call places the *system call number* in a designated location
   3. Library call executes a special instruction to switch from user
      mode to kernel mode (known as TRAP)
   4. Now in kernel mode, the appropriate system call handler is determined:
      1. Using the system call number as index
      2. This step is usually handled by a *dispatcher*
   5. System call handler is executed
   6. Control is returned to the library call, and switches from
      kernel mode to user mode
   7. Library call return to user program, via normal function
      return mechanism

** Switching between processes
*** Cooperative Approach
Processes transfer control of the CPU to the OS by making system
calls. The OS regains control of the CPU by waiting for a system call
or an illegal operation of some kind to take place.

*** Non-cooperative Approach
The question is: what ca the OS do to ensure that a rogue process
does not take over the machine?

The answer is: /timer interrupt/. A timer device is programmed to
raise an interrupt at a fixed interval. Each time the interrupt is
raised, a pre-configured interrupt handler in the OS runs.

At this time, the OS will decide whether to continue running the
process, or switch to a different one. This is the role of the
/scheduler/.

If the decision is to switch processes, then the OS executes a
low-level piece of code which is referred to as the /context
switch/. The OS saves a few register values for the current
executing process. This includes:

1. Program Counter (PC)
2. Stack Pointer (Pointing to the new context)

** Exception and Interrupts
Executing a machine level instruction can lead to exceptions, for
example arithmetic errors.

Exceptions are synchronous, and occur due to program execution. An
exception handle is executed automatically.

External events can interrupt the execution of a program. These are
usually hardware related: timer, keyboard events etc.

When an exception or an interrupt handler executes, control is
transferred to a handler routine automatically.

A handler does the following:

1. Save Register/CPU state
2. Perform Register/CPU
3. Restore Register/CPU
4. Return from interrupt

* Scheduling
Assumptions made:
1. Each job runs for the same amount of time
2. All jobs arrive at the same time
3. All jobs only use the CPU (i.e. they perform no I/O)
4. The run-time of each job is known

** Scheduling Metrics
1. Turn-around time
#+BEGIN_EXPORT latex
\begin{equation}
T_{turnaround} = T_{completion} - T_{arrival}
\end{equation}
#+END_EXPORT

** First Come First Served (FCFS)
Example:
- A, B and C arrived at time T=0.
- A runs first, followed by B, then C
Average Turnaround time:
(10 + 20 + 30)/3 = 20
*** Pros
Easy to implement
*** Cons
/Convoy effect/: a number of relatively-short potential consumers
of a resource get queued behind a heavyweight resource consumer.
 - E.g. A takes 100 TU, B and C 10
 - Average turnaround time: (100 + 110 + 120)/3
 - if instead, B and C were scheduled before A, it would be (10 + 20+
   120)/3
** Shortest Job First (SJF)
Schedule the job that takes the shortest TU.
*** Pros
Optimizes for Turnaround time
*** Cons
Relies on unrealistic assumptions. For example, if A takes 100TU, and
B and C takes 10 TU, but B and C arrive only shortly after A, then A
will still get queued, and the turnaround time will be high (convoy
problem again)
** Shortest Time-to-Completion First (PSTCF)
Any time a new job enters the system, it determines the job that has
the least time left, and schedules that one first.
*** Pros
Good turnaround time
*** Cons
Bad for response time and interactivity.
** Round Robin
Instead of running jobs to completion, RR runs a job for a /time
slice/, also sometimes called a /scheduling quantum/. After the time
slice, the next job in the run queue is scheduled. The length of the
time slice must be a multiple of the length of the timer-interrupt
period.

The shorter the time slice, the better the performance of RR under the
response-time metric. However, if the time slice is too short, there
will be a lot of overhead, and the cost of context switching will
dominate the overall performance.
*** Incorporating I/O
By treating each CPU burst as a job, the scheduler makes sure
processes that are "interactive" get run frequently.

** Multi-level Feedback Queue (MLFQ)
1. Optimise /turnaround time/.
2. Make the system responsive to interactive users, minimise /response
   time/.

How to schedule without perfect knowledge? (Knowing the length of the
job). Many jobs have phases of behaviour, and are thus predictable.

MLFQ has a number of distinct queues, each assigned a different
/priority level/. At any given time, a job that is ready to run is on
a single queue.

Rule 1: If Priority(A) > Priority(B), A runs
Rule 2: If Priority(A) = Priority(B), A and B run in RR

Note that job priority /changes/ over time.

First try at MLFQ:
- Rule 3: When a job enters the system, it is placed at the highest
  priority (the top most queue)
- Rule 4a: If a job uses up an entire time slice while running, its
  priority is /reduced/ (it moves down one queue)
- Rule 4b: If a job gives up the CPU before the time slice is up, it
  stays at the same /priority/ level.

Problems:
1. /starvation/: if there are "too many" interactive jobs in the
   system, they will combine to consume /all/ CPU time, and
   long-running jobs will never receive any CPU time.
2. /Gaming the scheduler/: One can stop using the CPU right before the
   time slice ends, then it will maintain at top priority.

Attempt 2:
- Rule 5: After some time period S, move all the jobs in the system to
  the topmost queue

This solves two problems:
1. Processes are guaranteed not to starve: by sitting in the top
   queue, a job will share the CPU with other high-priority jobs in a
   round-robin fashion, and will eventually receive service
2. If a CPU-bound job has become interactive, the scheduler treats it
   properly once it has received the priority boost

Attempt 3:
Instead of forgetting how much of a time slice a process used at a
given level, the scheduler should keep track, once a process has used
its allotment, it is demoted to the next priority queue.

- Rule 4: Once a job uses up its time allotment at a given level
  (regardless of how many times it has given up the CPU), its priority
  is reduced

*** Tuning MLFQ
1. Varying time-slice length across different queues. Shorter time
   slices are comprised of interactive jobs, and quickly alternating
   between them makes sense
2. The low-priority queues are CPU bound, and longer time slices work well.

** Lottery Scheduling
Tickets are used to represent the share of a resource that a process
should receive. Lottery scheduling achieves probabilistic fair sharing
of the CPU resources.

* Concurrency
Processes take a single physical CPU and turn it into multiple virtual
CPUs, enabling the illusion of multiple programs running at the same
time.

Now, we will examine the abstraction for running a single process:
that of a thread.
* Thread
The state of a single thread is similar to that of a process. It has a
program counter (PC) that tracks where the program is fetching
instructions from. Each thread has its own private set of registers it
uses for computation. If 2 threads are running on a single processor,
switching from a running one (T1) to running the other (T2) requires a
/context switch/. /Thread Control Blocks/ (TCBs) store the state of
each thread of a process. Unlike the context switch for processes, the
address space for threads remain the same.

[[file:./images/screenshot-01.png]]
** Example Thread creation

#+BEGIN_SRC c
  #include <stdio.h>
  #include <assert.h>
  #include <pthread.h>

  void *mythread(void *arg) {
    printf("%s\n", (char *) arg);
    return NULL; 
  }

  int main (int argc, char* argv[]) {
    pthread_t p1, p2;
    br int rc;
    printf("main: begin\n");
    rc = pthread_create(&p1, NULL, mythread, "A"); assert(rc == 0);
    rc = pthread_create(&p2, NULL, mythread, "B"); assert(rc == 0);
    //join waits for the threads to finish
    rc = pthread_join(p1, NULL); assert (rc == 0);
    rc = pthread_join(p2, NULL); assert (rc == 0);
    printf("main: end");
    return 0;
  }
#+END_SRC

** Issues with Uncontrolled Scheduling
*** Race Condition
Context switches that occur at untimely points in the execution can
result in the wrong result. Because multiple threads executing this
code can result in a race condition, we call this code a /critical
section/. What's required for this code to run properly is /mutual
exclusion/. This property guarantees that if one thread is executing
within the critical section, others will be prevented from doing so.
*** Key Terms
- Critical Section :: piece of code that accesses a /shared/ resource,
     usually a variable or data structure
- Race Condition :: A situation which arises if multiple threads of
                    execution enter the critical section at roughly
                    the same time; both attempt to update the shared
                    data structure at the same time, leading to
                    surprising and sometimes undesirable outcomes
- Indeterminate Program :: Consists of one or more race conditions;
     the output is non deterministic, something typically expected of
     computer programs
- Mutual Exclusion :: threads use /mutual exclusion/ primitives to
     avoid the problems that concurrency yields, such as race conditions
*** The wish for atomicity
What if we had a super-instruction like this:

#+BEGIN_SRC text
  memory-add 0x8044a1c, $0x1
#+END_SRC

Assume this instruction adds a value to a memory location, and the
hardware guarantees that it executes atomically. This would be easy if
the instruction set contained only 1 instruction. However, in the
general case this is not possible.

Instead, we ask the hardware for a few useful instructions upon which
we can build a general set of what is called /synchronisation
primitives/.
** Thread API
#+BEGIN_SRC c
  #include <pthread.h>

  int pthread_create (pthread_t * thread,
                      const pthread_attr_t* attr,
                      void * (*start_routine) (void *)
                      void * arg);
#+END_SRC

1. =thread= is a pointer to the structure of type =pthread_t=, used to
   interact with the thread
2. =attr= is used to specify attributes this thread might have,
   including setting the stack size, and scheduling priority of the
   thread. We can usually pass NULL in.
3. =start_routine= is the function this thread should start running in
4. =arg= is the argument =start_routine= requires.

#+BEGIN_SRC c
  int pthread_join(pthread_t trhead, void ** value_ptr);
#+END_SRC

=pthread_join= waits for the thread's completion.
*** Locks API
#+BEGIN_SRC c
  int pthread_mutex_lock(pthread_mutex_t *mutex);
  int pthread_mutex_unlock(pthread_mutex_t *mutex);

  // Usage
  //sets the lock to default values, making the lock usable
  pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;

  // dynamic way to do it is to make a call:
  int rc = pthread_mutex_init(&lock, NULL);
  assert (rc == 0); //always check success!

  pthread_mutex_lock(&lock);
  // Critical section
  x = x + 1;
  pthread_mutex_unlock(&lock);
#+END_SRC

#+BEGIN_SRC c
  int pthread_mutex_trylock(pthread_mutex_t *mutex);
  int pthread_mutex_timedlock(pthread_mutex_t *mutex,
                              struct timespec *abs_timeout);
#+END_SRC

These two calls are used in lock acquisition. =trylock= returns
failure if the lock is already held, and =timedlock= returns after a
timeout or after acquiring the lock, whichever happens first.
*** Condition Variables
#+BEGIN_SRC c
  int pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex);
  int pthread_cond_signal(pthread_cond_t *cond);
#+END_SRC

*condition variables* are useful when some kind of signalling must
 take place between threads.

#+BEGIN_SRC c
  pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
  pthread_cond_t init = PTHREAD_COND_INITIALIZER;

  int rc = pthread_mutex_lock(&lock); assert(rc == 0);
  while (initialized == 0) {
    int rc = pthread_cond_wait(&init, &lock);
    assert (rc == 0);
  }
  pthread_mutex_unlock(&lock);

  //Some other thread
  pthread_mutex_lock(&lock);
  initialized = 1;
  pthread_cond_signal(&init);
  pthread_mutex_unlock(&lock);
#+END_SRC
** Properties of Correct CS Implementation
- Mutual Exclusion :: If process P1 is executing in critical section,
     all other processes are prevented from entering the critical section
- Progress :: If no process is in a critical section, one of the
              waiting processes should be granted access
- Bounded Wait :: After process p1 request to enter critical section,
                  there exists an upperbound of number of times other
                  processes can enter the critical section before p1
- Independence :: Process *not* executing in critical section should
                 never block other process
** Locks
Calling the routine =lock()= tries to acquire the lock; if no other
thread holds the lock, the thread will acquire the lock and enter the
critical section; this thread is sometimes said to be the *owner* of
the lock.Once the *owner* of the lock calls =unlock()=, the lock in
now available again. If no othre threads are waiting for the lock
(i.e. no other thread has called =lock()= and is stuck), the state of
the lock is simply changed to free, if thee are waiting threads, one
of them will acquire the lock.
*** Pthread Locks

The name that the POSIX library uses for a lock is a *mutex*, as it is
used to provide *mutual exclusion* between threads. Different locks
can be initialized to protect different critical sections.
*** Evaluating locks
- mutual exclusion :: does the lock work, preventing multiple threads
     from entering a critical section?
- fairness :: Does each thread contending for the lock get a fair shot?
- performance :: Are the time overheads added by using the lock significant?
*** Approach 1: Controlling Interrupts
Using a special hardware instruction, turn off all interrupts during
critical section:

#+BEGIN_SRC c
  void lock() {
    DisableInterrupts(); 
  }

  void unlock() {
    EnableInterrupts();
  }
#+END_SRC
**** Pros
1. Simplicity
**** Cons
1. Requires calling thread to perform a /privileged/ operation
2. Doesn't work on multiprocessor systems
*** Approach 2: Test and Set
Hardware support for atomicity was created. This is known as the
*test-and-set instruction*, or *atomic exchange*.

The idea is to use a variable to indicate whether some thread has
possession of a lock. Calling =lock()= then tests and sets that variable.

However, this presents several issues:
1. *No Mutex*!
2. The thread waiting to acquire a lock is endlessly checking for the
   value of flag, a technique known as *spin-waiting*, which wastes
   time waiting for another thread to release a lock.

With hardware support for *test-and-set*, we achieve mutex, and have a
*spin lock*! To work correctly on a single processor, it requires a
preemptive scheduler, one that will interrupt a thread via  atimer, in
order to run a different thread, from time to time.
**** Evaluating the spin lock:
- correctness :: YES
- fairness :: NO, a thread may spin forever under contention
- performance ::  NO, high performance overheads

Other hardware primitives one can use to write locks:
1. LoadLinked and StoreConditional
2. Fetch-And-Add (ticket lock)
*** Two Phase Locks
A two-phase lock realises that spinning can be useful, particularly if
the lock is about to be released. In the first-phase, the lock spins
for a while, hoping that it can acquire a lock. However, if the lock
is not acquired during the first phase, the second phase is entered,
where the caller is put to sleep, and only woken up when the lock
becomes free later.
* Classical Synchronization Problems
*** Producer/Consumer
Producers and Consumers share a bounded buffer K

Blocking Version contains 3 semaphores:
1. Binary semaphore (initialized to 1) [mutex]
2. !Full (initialized to 4) [!Full]
3. !Empty (initialized to 0) [!Empty]

#+BEGIN_SRC c
  //Producer
  while (TRUE) {
    Produce Item;
    wait(notFull);
    wait(mutex);
    buffer[in] = item;
    in = (in + 1) % K;
    count++;
    signal(mutex);
    signal(notEmpty);
   }

  // Consumer
  while (TRUE) {
    wait(notEmpty);
    wait(mutex);
    item = buffer[out];
    out = (out + 1) % K;
    count--;
    signal(mutex);
    signal(notFull);
    Consume Item;
  }

#+END_SRC
*** Readers/Writers
Processes share a data structure D
1. Reader: Retrieves information from D
2. Writer: Modifies information from D
3. Writer must have exclusive access
4. Readers can read with other readers

#+BEGIN_SRC c
  while (true) {
    wait(roomEmpty);
    //Modifies data;
    signal(roomEmpty);
  }

  while (true) {
    wait(mutex);
    nReader++;
    if (nReader == 1) {
      wait(roomEmpty);
    }
    signal(mutex);

    // Reads data;
    wait(mutex);
    nReader --;
    if (nReader == 0) {
      signal(roomEmpty);
    }
    signal(mutex);
  }
#+END_SRC
*** Dining Philosophers
Philosophers sitting in a circle, requiring resource from both left
and right side.
**** Tanenbaum Solution
#+BEGIN_SRC C
  #define N 5
  #define LEFT i
  #define RIGHT ((i+1)%N)
  #define THINKING 1
  #define HUNGRY 1
  #define EATING 2

  int state[N];

  void philosopher(int i) {
    while(true) {
      think(); 
      hungry();
      takeChpStcks(i);
      eat();
      putChpStcks(i);
    }
  }
  void takeChpStcks(i) {
    wait(mutex);
    state[i] = HUNGRY;
    safeToEat(i);
    signal(mutex);
    wait(s[i]);
  }

  void safeToEat(i) {
    if (state[i] == HUNGRY &&
        state[LEFT] !=EATING &&
        state[RIGHT] != EATING) {
      state[i] = EATING;
      signal(s[i]);
    }
  }

  void putChpStcks(i) {
    wait(mutex);
    state[i] = THINKING;
    safeToEat(LEFT);
    safeToEat(RIGHT);
    signal(mutex);
  }
#+END_SRC
**** Limited Eater
Each chopstick has a semaphore of their own.
