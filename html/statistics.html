<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2017-12-02 Sat 21:54 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Statistics</title>
<meta name="generator" content="Org mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<div id="outline-container-org7f13cdc" class="outline-2">
<h2 id="org7f13cdc"><span class="section-number-2">1</span> Basic Properties</h2>
<div class="outline-text-2" id="text-1">
<ol class="org-ol">
<li>\(E(X) = \sum x p(x)\)</li>
<li>\(Var(X) = \sum (x-\mu)^2f(x)\)</li>
<li>X is around \(E(X)\), give or take \(SD(X)\)</li>
<li>\(E(aX + bY) = aE(X) + bE(Y)\)</li>
<li>\(Var(aX + bY) = a^2Var(X) + b^2Var(Y)\)</li>
<li>\(Var(X) = E(X^2) - [E(X)]^2\)</li>
<li>\(Cov(X_1, X_2) = E(X_1X_2) - E(X_1)E(X_2)\)</li>
<li>if \(X\), \(Y\) are independent:
<ol class="org-ol">
<li>\(M_{X+Y}(t) = M_X(t)M_Y(t)\)</li>
<li>\(E(XY)=E(X)E(Y)\), converse is true if \(X\) and \(Y\) are bivariate
normal, extends to multivariate normal</li>
</ol></li>
</ol>
</div>
</div>
<div id="outline-container-org9a4e335" class="outline-2">
<h2 id="org9a4e335"><span class="section-number-2">2</span> Approximations</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org6eb7bfa" class="outline-3">
<h3 id="org6eb7bfa"><span class="section-number-3">2.1</span> Law of Large Numbers</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Let \(X_1, X_2, ..., X_n\) be IID, with expectation \(\mu\) and variance
\(\sigma^2\). \(\overline{X_n} =
\frac{1}{n}\sum^{n}_{i=1}X_i\xrightarrow[n]{\infty}\mu\). Let \(x_1,
x_2, ..., x_n\) be realisations of the random variable \(X_1, X_2, ..., X_n\),
then \(\overline{x_n} = \frac{1}{n}\sum^{n}_{i=1}x_n
\xrightarrow[n]{\infty} \mu\)
</p>
</div>
</div>
<div id="outline-container-orgbeacb6a" class="outline-3">
<h3 id="orgbeacb6a"><span class="section-number-3">2.2</span> Central Limit Theorem</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Let \(S_n = \sum^{n}_{i=1}X_i\) where \(X_1, X_2, ..., X_n\) IID.
\(\frac{S_n - n\mu}{\sqrt{n}\sigma} \xrightarrow[n]{\infty} \mathcal{N}(0,1)\)
</p>
</div>
</div>
</div>
<div id="outline-container-org953a03d" class="outline-2">
<h2 id="org953a03d"><span class="section-number-2">3</span> Distributions</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org9a19984" class="outline-3">
<h3 id="org9a19984"><span class="section-number-3">3.1</span> Poisson(\(\lambda\))</h3>
<div class="outline-text-3" id="text-3-1">
<p>
\(E(X) = Var(X) = \lambda\)
</p>
</div>
</div>
<div id="outline-container-orgdbc17c9" class="outline-3">
<h3 id="orgdbc17c9"><span class="section-number-3">3.2</span> Normal \(X \sim \mathcal{N}(\mu, \sigma^2)\)</h3>
<div class="outline-text-3" id="text-3-2">
<p>
\(f(x) = \frac{1}{\sqrt{2\pi}\sigma} exp
\left(-\frac{(x-\mu)^2}{2\sigma^2}\right), -\infty<x<\infty\)
</p>
<ol class="org-ol">
<li>When \(\mu = 0\), \(f(x)\) is an even function, and \(E(X^k) = 0\) where
\(k\) is odd</li>
<li>\(Y = \frac{X-E(X)}{SD(X)}\) is the standard normal</li>
</ol>
</div>
</div>
<div id="outline-container-orgae8266f" class="outline-3">
<h3 id="orgae8266f"><span class="section-number-3">3.3</span> Gamma \(\Gamma\)</h3>
<div class="outline-text-3" id="text-3-3">
<p>
\(g(t) = \frac{\lambda^\alpha}{\Gamma(\alpha)}t^{\alpha-1}e^{-\lambda
t}, t \ge 0\)
</p>

<p>
\(\mu_1 = \frac{\alpha}{\lambda}, \mu_2 = \frac{\alpha(\alpha+1)}{\lambda^2}\)
</p>
</div>
</div>

<div id="outline-container-org2576c60" class="outline-3">
<h3 id="org2576c60"><span class="section-number-3">3.4</span> \(\chi^2\) Distribution</h3>
<div class="outline-text-3" id="text-3-4">
<p>
Let \(\mathcal{Z} \sim \mathcal{N}(0,1)\), \(\mathcal{U} =
\mathcal{Z}^2\) has a \(\chi^2\) distribution with 1 d.f. 
</p>

<p>
\(f_{\mathcal{U}}(u) = \frac{1}{\sqrt{2\pi}} u^{-\frac{1}{2}}
e^{-\frac{u}{2}}, u \ge 0\)
</p>

<p>
\(\chi_1^2 \sim \Gamma(\alpha=\frac{1}{2}, \lambda=\frac{1}{2})\)
</p>

<p>
Let \(U_1, U_2, ..., U_n\) be \(\chi_1^2\) IID, then \(V=\sum^{n}_{i=1}U_i\)
is \(\chi_n^2\) with n degree freedom, \(V \sim
\Gamma(\alpha=\frac{n}{2}, \lambda=\frac{1}{2})\)
</p>

<p>
\(E(\chi_n^2) = n, Var(\chi_n^2) = 2n\)
</p>

<p>
\(M(t) = \left(1 - 2t\right)^{-\frac{n}{2}}\)
</p>
</div>
</div>
<div id="outline-container-orge43b88a" class="outline-3">
<h3 id="orge43b88a"><span class="section-number-3">3.5</span> t-distribution</h3>
<div class="outline-text-3" id="text-3-5">
<p>
Let \(\mathcal{Z} \sim \mathcal{N}(0,1)\), \(\mathcal{U}_n \sim
\chi_n^2\) be independent, \(t_n = \frac{\mathcal{Z}}{\sqrt{U_n / n}}\) has a t-distribution with n d.f.
</p>

<p>
\(f(t) = \frac{\Gamma([(n+1)/2])}{\sqrt{n}\pi\Gamma(n/2)}\left(1 +
\frac{t^2}{n} \right)^{-\frac{n+1}{2}}\)
</p>
<ol class="org-ol">
<li>t is symmetric about 0</li>
<li>\(t_n \xrightarrow[n]{\infty} \mathcal{Z}\)</li>
</ol>
</div>
</div>
<div id="outline-container-org491b4cf" class="outline-3">
<h3 id="org491b4cf"><span class="section-number-3">3.6</span> F-distribution</h3>
<div class="outline-text-3" id="text-3-6">
<p>
Let \(U \sim \chi_m^2, V \sim \chi_n^2\) be independent, \(W =
\frac{U/m}{V/n}\) has an F distribution with (m,n) d.f.
</p>

<p>
If \(X \sim t_n\), \(X^2 = \frac{\mathcal{Z}/1}{U_n/n}\) is an F
distribution with (1,n) d.f, with \(w \ge 0\):
</p>

<p>
For \(n > 2\), \(E(W) = \frac{n}{n-2}\)
</p>
</div>
</div>
</div>
<div id="outline-container-orgd6d2ab4" class="outline-2">
<h2 id="orgd6d2ab4"><span class="section-number-2">4</span> Sampling</h2>
<div class="outline-text-2" id="text-4">
<p>
Let \(X_1, X_2, ..., X_n\) be IID \(\mathcal{N}(\mu, \sigma^2)\).
</p>

<p>
\(\text{sample mean, } \overline{X} = \frac{1}{n}\sum^{n}_{i=1}X_i\)
</p>

<p>
\(\text{sample variance, } S^2 = \frac{1}{n-1}\sum^{n}_{i=1}\left(X_i-\overline{X}\right)^2\)
</p>
</div>
<div id="outline-container-orgd5bbb48" class="outline-3">
<h3 id="orgd5bbb48"><span class="section-number-3">4.1</span> Properties of \(\overline{X}\) and \(S^2\)</h3>
<div class="outline-text-3" id="text-4-1">
<ol class="org-ol">
<li>\(\overline{X}\) and \(S^2\) are independent</li>
<li>\(\overline{X} \sim \mathcal{N}(\mu, \frac{\sigma^2}{n})\)</li>
<li>\(\frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2\)</li>
<li>\(\frac{\overline{X} - \mu}{S/\sqrt{n}} \sim t_{n-1}\)</li>
</ol>
</div>
</div>
<div id="outline-container-org791ae6d" class="outline-3">
<h3 id="org791ae6d"><span class="section-number-3">4.2</span> Simple Random Sampling (SRS)</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Assume \(n\) random draws are made without replacement. (Not SRS, will
be corrected for later).
</p>
</div>
<div id="outline-container-org0594e5a" class="outline-4">
<h4 id="org0594e5a"><span class="section-number-4">4.2.1</span> Summary of Lemmas</h4>
<div class="outline-text-4" id="text-4-2-1">
<ul class="org-ul">
<li>\(P(X_i =\xi_j) = \frac{n_j}{N}\): Lemma A</li>
<li>For \(i \ne j\), \(Cov(X_i, X_j) = - \frac{\sigma^2}{N-1}\): Lemma B</li>
</ul>
</div>
</div>
<div id="outline-container-orgc33364a" class="outline-4">
<h4 id="orgc33364a"><span class="section-number-4">4.2.2</span> Estimation Problem</h4>
<div class="outline-text-4" id="text-4-2-2">
<p>
Let \(X_1, X_2, ..., X_n\) be random draws with replacement. Then
\(\overline{X}\) is an estimator of \(\mu\). and the observed value of
\(\overline{X}\), \(\overline{x}\) is an estimate of \(\mu\).
</p>
</div>
</div>
<div id="outline-container-org9c7604a" class="outline-4">
<h4 id="org9c7604a"><span class="section-number-4">4.2.3</span> Standard Error (SE)</h4>
<div class="outline-text-4" id="text-4-2-3">
<p>
SE of an \(\overline{X}\) is defined to be \(SD(\overline{X})\).
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">param</td>
<td class="org-left">est</td>
<td class="org-left">SE</td>
<td class="org-left">Est. SE</td>
</tr>

<tr>
<td class="org-left">\(\mu\)</td>
<td class="org-left">\(\overline{X}\)</td>
<td class="org-left">\(\frac{\sigma}{\sqrt{n}}\)</td>
<td class="org-left">\(\frac{s}{\sqrt{n}}\)</td>
</tr>

<tr>
<td class="org-left">\(p\)</td>
<td class="org-left">\(\hat{p}\)</td>
<td class="org-left">\(\sqrt{\frac{p(1-p)}{n}}\)</td>
<td class="org-left">\(\sqrt{\frac{\hat{p}(1-\hat{p})}{n-1}}\)</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-org83834d8" class="outline-4">
<h4 id="org83834d8"><span class="section-number-4">4.2.4</span> Without Replacement</h4>
<div class="outline-text-4" id="text-4-2-4">
<p>
SE is multiplied by \(\frac{N-n}{N-1}\), because \(s^2\) is biased for
\(\sigma^2\): \(E(\frac{N-1}{N}s^2) = \sigma^2\), but N is normally large.
</p>
</div>
</div>
<div id="outline-container-org97c0850" class="outline-4">
<h4 id="org97c0850"><span class="section-number-4">4.2.5</span> Confidence Interval</h4>
<div class="outline-text-4" id="text-4-2-5">
<p>
An approximate \(1-\alpha\) CI for \(\mu\) is
</p>

<p>
\((\overline{x} - z_{\alpha/2}\frac{s}{\sqrt{n}}, \overline{x} + z_{\alpha/2}\frac{s}{\sqrt{n}})\)
</p>
</div>
</div>
</div>
<div id="outline-container-orga1005e8" class="outline-3">
<h3 id="orga1005e8"><span class="section-number-3">4.3</span> Biased Measurements</h3>
<div class="outline-text-3" id="text-4-3">
<p>
Let \(X = \mu + \epsilon\), where \(E(\epsilon) = 0\), \(Var(\epsilon) =
\sigma^2\)
</p>

<p>
Suppose X is used to measure an unknown constant a, \(a \ne \mu\). \(X =
a + (\mu - a) + \epsilon\), where \(\mu-a\) is the bias.
</p>

<p>
Mean square error (MSE) is \(E((X-a)^2) = \sigma^2 + (\mu - a)^2\)
</p>

<p>
with n IID measurements, \(\overline{x} = \mu + \overline{\epsilon}\)
</p>

<p>
\(E((x - a)^2) = \frac{\sigma^2}{n} + \left(\mu - a\right)^2\)
</p>

<p>
\(\text{MSE} = \text{\text{SE}}^2 + \text{bias}^2\), hence
\(\sqrt{\text{MSE}}\) is a good measure of the accuracy of the estimate
\(\overline{x}\) of a.
</p>
</div>
</div>
<div id="outline-container-orge40196f" class="outline-3">
<h3 id="orge40196f"><span class="section-number-3">4.4</span> Estimation of a Ratio</h3>
<div class="outline-text-3" id="text-4-4">
<p>
Consider a population of \(N\) members, and two characteristics are
recorded: \((X_1, Y_1), (X_2, Y_2), ... , (X_n, Y_n)\), \(r =
\frac{\mu_y}{\mu_x}\).
</p>

<p>
An obvious estimator of r is \(R = \frac{\overline{Y}}{\overline{X}}\)
</p>

<p>
\(Cov(\overline{X},\overline{Y}) = \frac{\sigma_{xy}}{n}\), where
</p>

<p>
\(\sigma_{xy} := \frac{1}{N}\sum^{N}_{i=1}(x_i-\mu_x)(x_i-\mu_y)\) is
the population covariance.
</p>
</div>
<div id="outline-container-org0b6c9c9" class="outline-4">
<h4 id="org0b6c9c9"><span class="section-number-4">4.4.1</span> Properties</h4>
<div class="outline-text-4" id="text-4-4-1">
<p>
\(Var(R) &\approx \frac{1}{\mu_x^2}\left(r^2\sigma_{\overline{X}}^2 + \sigma_{\overline{Y}}^2 - 2r\sigma_{\overline{X}\overline{Y}}\right)\)
</p>

<p>
Population coefficient \(\rho =
\frac{\sigma_{xy}}{\sigma_{x}\sigma_{y}}\)
</p>

<p>
\(E(R) \approx r + \frac{1}{n}\left(\frac{N-n}{N-1}\right)\frac{1}{\mu_x^2}\left(r\sigma_x^2-\rho\sigma_x\sigma_y\right)\)
</p>

<p>
\(s_{xy} = \frac{1}{n-1}\sum^{n}_{i=1}\left(X_i -
\overline{X}\right)\left(Y_i - \overline{Y}\right)\)
</p>
</div>
</div>
<div id="outline-container-orgffdcd14" class="outline-4">
<h4 id="orgffdcd14"><span class="section-number-4">4.4.2</span> Ratio Estimates</h4>
<div class="outline-text-4" id="text-4-4-2">
<p>
\(\overline{Y}_R = \frac{\mu_x}{\overline{X}}\overline{Y} = \mu_xR\)
</p>

<p>
\(Var(\overline{Y}_R) \approx
\frac{1}{n}\frac{N-n}{N-1}(r^2\sigma_x^2 + \sigma_y^2
-2r\rho\sigma_x\sigma_y)\)
</p>

<p>
\(E(\overline{Y}_R) - \mu_y \approx
\frac{1}{n}\frac{N-n}{N-1}\frac{1}{\mu_x}\left(r\sigma_x^2 -\rho\sigma_x\sigma_y\right)\)
</p>

<p>
The bias is of order \(\frac{1}{n}\), small compared to its standard error.
</p>

<p>
\(\overline{Y}_R\) is better than \(\overline{Y}\), having smaller
variance, when \(\rho > \frac{1}{2}\left(\frac{C_x}{C_y}\right)\), where
\(C_i = \sigma_i/\mu_i\)
</p>

<p>
Variance of \(\overline{Y}_R\) can be estimated by
</p>

<p>
\(s_{\overline{Y}_R}^2 =
\frac{1}{n}\frac{N-n}{N-1}\left(R^2s_x^2+s_y^2-2Rs_{xy}\right)\)
</p>

<p>
An approximate \(1-\alpha\) C.I. for \(\mu_y\) is \(\overline{Y}_R \pm
z_{\alpha/2}s_{\overline{Y}_R}\)
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org921cbbf" class="outline-2">
<h2 id="org921cbbf"><span class="section-number-2">5</span> Method of Moments</h2>
<div class="outline-text-2" id="text-5">
<p>
To estimate \(\theta\), express it as a function of moments
\(g(\hat{\mu}_1,\hat{\mu}_2,...)\)
</p>
</div>
<div id="outline-container-org2e0ee08" class="outline-3">
<h3 id="org2e0ee08"><span class="section-number-3">5.1</span> Monte Carlo</h3>
<div class="outline-text-3" id="text-5-1">
<p>
<b>Monte Carlo</b> is used to generate many realisations of random
variable.
</p>

<p>
\(\overline{X} \xrightarrow[n]{\infty} \alpha/\lambda, \hat{\sigma}^2
 \xrightarrow[n]{\infty}\alpha/\lambda^2\), MOM estimators are
consistent (asymptotically unbiased).
</p>

<p>
\(\text{Poisson}(\lambda)\): \(\text{bias} = 0, SE \approx \sqrt{\frac{\overline{x}}{n}}\)
</p>

<p>
\(N(\mu, \sigma^2)\): \(\mu = \mu_1\), \(\sigma^2 = \mu_2 - \mu_1^2\)
</p>

<p>
\(\Gamma(\lambda, \alpha)\): \(\hat{\lambda} =
 \frac{\hat{\mu}_1}{\hat{\mu}_2-\hat{\mu}_1^2}=\frac{\overline{X}}{\hat{\sigma}^2}, \hat{\alpha} = \frac{\hat{\mu}_1^2}{\hat{\mu}_2-\hat{\mu}_1^2}=\frac{\overline{X}^2}{\hat{\sigma}^2}\)
</p>
</div>
</div>
</div>
<div id="outline-container-org0652dbc" class="outline-2">
<h2 id="org0652dbc"><span class="section-number-2">6</span> Maximum Likelihood Estimator (MLE)</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-orgf0d4093" class="outline-3">
<h3 id="orgf0d4093"><span class="section-number-3">6.1</span> Poisson Case</h3>
<div class="outline-text-3" id="text-6-1">
<p>
\(L(\lambda) = \prod^n_{i=1}\frac{\lambda^{x_i}e^{-\lambda}}{x_i!} = \frac{\lambda\sum^n_{i=1}x_ie^{-n\lambda}}{\prod^{n}_{i=1}x_i!}\)
</p>

<p>
\(l(\lambda) = \sum^{n}_{i=1}x_i\log\lambda - n\lambda -
\sum^{n}_{i=1}\log x_i!\)
</p>

<p>
ML estimate of \(\lambda_0\) is \(\overline{x}\). ML estimator is
\(\hat{\lambda}_0 = \overline{X}\)
</p>
</div>
</div>
<div id="outline-container-orgfaf0878" class="outline-3">
<h3 id="orgfaf0878"><span class="section-number-3">6.2</span> Normal case</h3>
<div class="outline-text-3" id="text-6-2">
<p>
\(l(\mu, \sigma) = -n\log\sigma - \frac{n\log 2\pi}{2} - \frac{\sum^{n}_{i=1}\left(X_i-\mu\right)^2}{2\sigma^2}\)
</p>

<p>
\(\frac{\partial l}{\partial \mu} = \frac{\sum \left(X_i -
\mu\right)}{\sigma^2} \implies \hat{\mu} = \overline{x}\)
</p>

<p>
\(\frac{\partial l}{\partial \sigma} =
\frac{\sum^{n}_{i=1}\left(X_i-\mu\right)^2}{\sigma^3} -
\frac{n}{\sigma} \\ \implies \hat{\sigma^2} = \frac{1}{n}\sum^{n}_{i=1}\left(X_i-\overline{X}\right)^2\)
</p>
</div>
</div>
<div id="outline-container-org505fa65" class="outline-3">
<h3 id="org505fa65"><span class="section-number-3">6.3</span> Gamma case</h3>
<div class="outline-text-3" id="text-6-3">
<p>
\(l(\theta) = n\alpha\log\lambda + (\alpha -1)\sum^{n}_{i=1}\log X_i -
\lambda\sum^{n}_{i=1} X_i - n\log\Gamma(\alpha)\)
</p>

<p>
\(\frac{\partial l}{\partial \alpha} = n\log\alpha + \sum^{n}_{i=1}\log
X_i - \sum^{n}_{i=1}X_i - \frac{n}{\Gamma(\alpha)}\Gamma '(\alpha)\)
</p>

<p>
\(\frac{\partial l}{\partial \lambda} = \frac{n\alpha}{\lambda} -
\sum^{n}_{i=1}X_i\)
</p>

<p>
\(\hat{\lambda} = \frac{\hat{\alpha}}{\hat{x}}\)
</p>
</div>
</div>
<div id="outline-container-org7464d8f" class="outline-3">
<h3 id="org7464d8f"><span class="section-number-3">6.4</span> Multinomial Case</h3>
<div class="outline-text-3" id="text-6-4">
<p>
\(f(x_1, ..., x_r) = {n \choose {x_1, x_2, ... x_r}} \prod^{n}_{i=1}
p_i^{X_i}\)
</p>

<p>
where \(X_i\) is the number of times the value occurs, and not the
number of trials. and \(x_1, x_2, ... x_r\) are non-negative integers
summing to \(n\). \(\forall i\):
</p>

<p>
\(E(X_i) = np_i, Var(X_i)=np_i(1-p_i)\)
</p>

<p>
\(Cov(X_i,X_j) = -np_ip_j, \forall i \ne j\)
</p>

<p>
\(l(p) = \Kappa + \sum^{r-1}_{i=1}x_i\log p_i +
x_r\log(1-p_1-...-p_{r-1})\)
</p>

<p>
\(\frac{\partial l}{\partial p_i} = \frac{x_i}{p_i} - \frac{x_r}{p_r} =
0 \text{ assuming MLE exists}\)
</p>

<p>
\(\frac{x_i}{\hat{p}_i} = \frac{x_r}{\hat{p}_r} \implies \hat{p}_i =
\frac{x_i}{c}, c=\frac{x_r}{\hat{p}_r}\)
</p>

<p>
\(\sum^r_{i=1}\hat{p}_i = \sum^r_{i=1}\frac{x_i}{c} = 1 \\ \implies c =
\sum^{r}_{i=1}x_i = n \implies \hat{p}_i = \frac{\overline{x}_i}{n}\)
</p>

<p>
same as MOM estimator.
</p>
</div>
</div>
<div id="outline-container-org478467d" class="outline-3">
<h3 id="org478467d"><span class="section-number-3">6.5</span> CIs in MLE</h3>
<div class="outline-text-3" id="text-6-5">
<p>
\(\frac{\hat{X} - \mu}{s/\sqrt{n}} \sim t_{n-1}\)
</p>

<p>
Given the realisations \(\overline{x}\) and \(s\), \(\overline{x} \pm
t_{n-1, \alpha/2}\frac{s}{\sqrt{n}},\overline{x} + t_{n-1,
\alpha/2}\frac{s}{\sqrt{n}}\) is the exact \(1-\alpha\) CI for \(\mu\).
</p>

<p>
\(\frac{n\hat{\sigma}^2}{\sigma^2} \sim \chi_{n-1}^\),
\(\frac{n\hat{\sigma}^2}{\chi_{n-1,\alpha/2}^2},
\frac{n\hat{\sigma}^2}{\chi_{n-1,1-\alpha/2}^2}\) is the exact
\(1-\alpha\) CI for \(\sigma\).
</p>
</div>
</div>
</div>
<div id="outline-container-orgb923047" class="outline-2">
<h2 id="orgb923047"><span class="section-number-2">7</span> Fisher Information</h2>
<div class="outline-text-2" id="text-7">
<p>
\(I\left( \theta \right) = - E \left( \frac{\partial}{\partial \theta^2} \log
    f\left( x | \theta \right) \right)\)
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Distribution</th>
<th scope="col" class="org-left">MLE</th>
<th scope="col" class="org-left">Variance</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Po(\(\lambda\))</td>
<td class="org-left">\(X\)</td>
<td class="org-left">\(\lambda\)</td>
</tr>

<tr>
<td class="org-left">Be(\(p\))</td>
<td class="org-left">\(X\)</td>
<td class="org-left">\(p\left(1-p\right)\)</td>
</tr>

<tr>
<td class="org-left">Bin(\(n\),\(p\))</td>
<td class="org-left">\(\frac{X}{n}\)</td>
<td class="org-left">\(\frac{p(1-p)}{n}\)</td>
</tr>

<tr>
<td class="org-left">HWE tri</td>
<td class="org-left">\(\frac{X_2+2X_3}{n}\)</td>
<td class="org-left">\(\frac{\theta(1-\theta)}{n}\)</td>
</tr>
</tbody>
</table>

<p>
General trinomial: \(\left(\frac{X_1}{n}, \frac{X_2}{n} \right)\)
</p>

\begin{equation*}
\begin{bmatrix} p_1(1-p_1) & -p_1p_2 \\ -p_1p_2 & p_2(1-p_2) \end{bmatrix} \frac{1}{n}
\end{equation*}

<p>
In all the above cases, \(\text{var}(\hat{\theta}) = I(\theta)^{-1}\).
</p>
</div>
</div>
<div id="outline-container-org602bb3e" class="outline-2">
<h2 id="org602bb3e"><span class="section-number-2">8</span> Asymptotic Normality of MLE</h2>
<div class="outline-text-2" id="text-8">
<p>
As \(n \rightarrow \infty\), \(\sqrt{nI(\theta)}(\hat{\theta} -
\theta) \rightarrow N(0,1)\) in distribution, and hence \(\hat{\theta}
\sim N\left(\theta, \frac{I\left( \theta \right)^{-1}}{n}\right)\)
</p>

<p>
As \(\hat{\theta} \xrightarrow[n]{\infty} \theta\), MLE is consistent.
</p>

<p>
SE of an estimate of \(\theta\) is the SD of the estimator
\(\hat{\theta}\), hence \(SE = SD(\hat{\theta}) =
\sqrt{\frac{I(\theta)^{-1}}{n}} \approx
\sqrt{\frac{I(\hat{\theta})^{-1}}{n}}\)
</p>

<p>
\(1-\alpha \text{ CI } \approx \hat{\theta} \pm
  z_{\alpha/2}\sqrt{\frac{I(\theta)^{-1}}{n}}\)
</p>
</div>
</div>
<div id="outline-container-org971aa87" class="outline-2">
<h2 id="org971aa87"><span class="section-number-2">9</span> Efficiency</h2>
<div class="outline-text-2" id="text-9">
<p>
Cramer-Rao Inequality: if \(\theta\) is unbiased, then \(\forall \theta
\in \Theta\) , \(var(\hat{\theta}) \ge I(\hat{\theta})^{-1}/n\), if =
then \(\hat{\theta}\) is efficient.
</p>

<p>
\(eff(\hat{\theta}) = \frac{I(\hat{\theta})^{-1}/n}{var(\hat{\theta})}< 1\)
</p>
</div>
</div>
<div id="outline-container-orgf563bde" class="outline-2">
<h2 id="orgf563bde"><span class="section-number-2">10</span> Sufficiency</h2>
<div class="outline-text-2" id="text-10">
</div>
<div id="outline-container-orged5836e" class="outline-3">
<h3 id="orged5836e"><span class="section-number-3">10.1</span> Characterisation</h3>
<div class="outline-text-3" id="text-10-1">
<p>
Let \(S_t = {x: T(x) = t}\). The sample space of \(X\), \(S\) is the
disjoint union of \(S_t\) across all possible values of \(T\).
</p>

<p>
\(T\) is sufficient for \(\theta\) if \(\exists q() \text{ s.t. } \forall x \in S_t,
f_{\theta}(X\x|T=t) = q(x)\).
</p>
</div>
</div>
<div id="outline-container-orge850d60" class="outline-3">
<h3 id="orge850d60"><span class="section-number-3">10.2</span> Factorisation Theorem</h3>
<div class="outline-text-3" id="text-10-2">
<p>
\(T\) is sufficient for  \(\theta\) iff \(\exists g(t,\theta), h(x)
\text{ s.t. } \forall \theta \in \Theta, f_\theta(x) = g(T(x), \theta) h(x)
\forall x\)
</p>
</div>
</div>
<div id="outline-container-orgc4359e2" class="outline-3">
<h3 id="orgc4359e2"><span class="section-number-3">10.3</span> Rao-Blackwell Theorem</h3>
<div class="outline-text-3" id="text-10-3">
<p>
Let \(\hat{\theta}\) be an estimator of \(\theta\) with finite variance,
\(T\) be sufficient for \(\theta\). Let \(\tilde{\theta} =
E[\hat{\theta}|T]\). Then for every \(\theta \in \Theta\),
\(E\left(\hat{\theta} - \theta\right)^2 \le
E\left(\hat{\theta}-\theta\right)^2\). Equality holds iff
\(\hat{\theta}\) is a function of \(T\).
</p>
</div>
</div>
<div id="outline-container-orgba516ee" class="outline-3">
<h3 id="orgba516ee"><span class="section-number-3">10.4</span> Random Conditional Expectation</h3>
<div class="outline-text-3" id="text-10-4">
<ol class="org-ol">
<li>\(E(X) = E(E(X|T))\)</li>
<li>\(var(X) = var(E(X|T)) + E(var(X|T))\)</li>
<li>\(var(Y|X) =E(Y^2|X) - E(Y|X)^2\)</li>
<li>\(E(Y) = Y, var(Y) =0\) iff \(Y\) is a constant</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org195c7a8" class="outline-2">
<h2 id="org195c7a8"><span class="section-number-2">11</span> Hypothesis Testing</h2>
<div class="outline-text-2" id="text-11">
<p>
Let \(X_1... X_n\) be IID with density \(f(x|\theta)\). null \(H_0: \theta
= \theta_0\), \(H-1 : \theta = \theta_1\). Critical region is
\(R\subsetR_n\). \(size = P_0(X \in R)\) and \(power = P_1(X\in R)\).
</p>

<p>
\(\Lambda(x) = \frac{f_0(x_1)...f_0(x_n)}{f_1(x_1)...f_1(x_n)}\).
Critical region \({x : \Lambda(x) < c_\alpha}\), and among all tests
with this size, it has the maximum power (Neyman-Pearson Lemma).
</p>

<p>
A hypothesis is simple if it completely specifies the distibution of
the data.
</p>

<p>
\(H_1 : \mu > \mu_0\):  Critical region \(\{\bar{x} > \mu_0 +
z_\alpha\frac{\sigma}{\sqrt{n}}\}\), the power is a function of \(\mu\),
and this is uniformly the most powerful test for size \(\le \alpha\).
</p>

<p>
\(H_1 : \mu \ne \mu_0\): Critical region \(\{|\bar{x}-\mu_0| > c\}, c =
z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}\), but not uniformly most
powerful.
</p>

<p>
The \((1-\alpha)\) CI for \(\mu\) consists of precisely the values \(\mu_0\)
for which \(H_0: \mu = \mu_0\) is not rejected against \(H_1: \mu \ne
\mu_0\). Exact for normal with known variance, approx. in others.
</p>
</div>

<div id="outline-container-org4dabe81" class="outline-3">
<h3 id="org4dabe81"><span class="section-number-3">11.1</span> p-value</h3>
<div class="outline-text-3" id="text-11-1">
<p>
the probability under \(H_0\) that the test statistic is more extreme
than the realisation. (A, B): \(p = p_0(\bar{X} > \bar{x}) =
P(Z>\frac{\bar{x} - \mu_0}{\sigma/\sqrt{n}})\). (C): \(p =
P_0(|\bar{X} - \mu_0| > |\bar{x} - \mu_0|)\). The smaller the p-value,
the more suspicious one should be about \(H_0\). If size is smaller than
p-value, do not reject \(H_0\).
</p>
</div>
</div>
</div>

<div id="outline-container-org6e6af00" class="outline-2">
<h2 id="org6e6af00"><span class="section-number-2">12</span> Generalized Likelihood Ratio</h2>
<div class="outline-text-2" id="text-12">
<p>
\(\Lambda^* = \frac{\text{max}_{\theta \in
\omega_0}L(\theta)}{\text{max}_{\theta\in\Omega}L(\theta)}\), \(\Omega =
\omega_0 \cup \omega_1\). The closer \(\Lambda\) is to 0, the stronger
the evidence for \(H_1\).
</p>
</div>

<div id="outline-container-orge6c7c9f" class="outline-3">
<h3 id="orge6c7c9f"><span class="section-number-3">12.1</span> Large-sample null distribution of \(\Lambda\)</h3>
<div class="outline-text-3" id="text-12-1">
<p>
Under \(H_0\), when n is large, \(-2\log\Lambda = \chi_k^2\), where \(k =
\text{dim}(\Omega) - \text{dim}(\omega_0)\).
</p>

<p>
Normal (C): \(p = P\left(\chi_1^2 > \frac{(\bar{x} -
\mu_0)^2}{\sigma^2/n}\right)\)
</p>

<p>
Multinomial: \(\Lambda = \prod_{i=1}^{r}
\left(\frac{E_i}{X_i}\right)^{X_i}\) where \(E_i = np_i(\hat{\theta})\) is
the expected frequency of the ith event under \(H_0\). \(-2\log\Lambda
\approx \sum_{i=1}^{r}\frac{(X_i-E_i)^2}{E_i}\), which is the Pearson
chi-square statistic, written as \(X^2\).
</p>
</div>
</div>

<div id="outline-container-org363ff76" class="outline-3">
<h3 id="org363ff76"><span class="section-number-3">12.2</span> Poisson Dispersion Test</h3>
<div class="outline-text-3" id="text-12-2">
<p>
For \(i = 1 ... n\) let \(X_i \sim Poisson(\lambda_i)\) are independent.
</p>

<p>
\(w_0 = \{ \tilde{\lambda} |  \lambda_1 = \lambda_2 = ... =
\lambda_n\}\)
</p>

<p>
\(w_1 = \{\tilde{\lambda} | \lambda_i \ne \lambda_j \text{ for some }
i,j\}\)
</p>

<p>
\(-2\log\Lambda \approx \frac{\sum_{i=1}^{n}(X_i-\bar{X})^2}{\bar{X}}\).
For large n, the null distribution of \(-2\log\Lambda\) is approximately
\(\chi_{n-1}^2\)
</p>
</div>
</div>
</div>

<div id="outline-container-orgb49efe5" class="outline-2">
<h2 id="orgb49efe5"><span class="section-number-2">13</span> Comparing 2 samples</h2>
<div class="outline-text-2" id="text-13">
</div>
<div id="outline-container-orgc1fcdd3" class="outline-3">
<h3 id="orgc1fcdd3"><span class="section-number-3">13.1</span> Normal Theory: Same Variance</h3>
<div class="outline-text-3" id="text-13-1">
<p>
\(X_1, ..., X_n\) be i.i.d \(N(\mu_X,\sigma^2)\) and \(Y_1,...,Y_m\) be
i.i.d \(N(\mu_Y, \sigma^2)\), independent. \(H_0: \mu_X - \mu_Y = d\)
</p>
</div>
<div id="outline-container-org06f3e19" class="outline-4">
<h4 id="org06f3e19"><span class="section-number-4">13.1.1</span> Known Variance</h4>
<div class="outline-text-4" id="text-13-1-1">
<p>
\(Z := \frac{\bar{X} - \bar{Y} - (\mu_X -
\mu_Y)}{\sigma{\sqrt{\frac{1}{n} + \frac{1}{m}}}}\) and reject \(H_0\)
when \(|Z| > z_{\alpha/2}\)
</p>
</div>
</div>
<div id="outline-container-org2a0f2da" class="outline-4">
<h4 id="org2a0f2da"><span class="section-number-4">13.1.2</span> Unknown Variance</h4>
<div class="outline-text-4" id="text-13-1-2">
<p>
\(s_p^2 = \frac{(n-1)s_X^2 + (m-1)s_Y^2}{m+n-2}\) where $s<sub>X</sub><sup>2</sup> =
\frac{1}{n-1}&sum;<sub>i=1</sub><sup>n</sup>(X<sub>i</sub>-\bar{X})<sup>2</sup>. \(s_p^2\) is an unbiased
estimator of \(\sigma^2\). \(s_X\) within factor of 2 from \(s_Y\).
</p>

<p>
\(t := \frac{\bar{X} - \bar{Y} - (\mu_X -
\mu_Y)}{s_p{\sqrt{\frac{1}{n} + \frac{1}{m}}}}\) follows a t
distribution with \(m+n-2\) d.f.
</p>

<p>
If two-sided: reject \(H_0\) when \(|t| > t_{n+m-2,\alpha/2}\). If
one-sided, e.g \(H_1: \mu_X > \mu_Y\), reject \(H_0\) when \(t >
t_{n+m-2,\alpha}\). 
</p>
</div>
</div>
<div id="outline-container-orge62f8d7" class="outline-4">
<h4 id="orge62f8d7"><span class="section-number-4">13.1.3</span> CI</h4>
<div class="outline-text-4" id="text-13-1-3">
<p>
\(\frac{\bar{X}-\bar{Y}}\pm z_{\alpha/2} \cdot \sigma
\sqrt{\frac{1}{n} + \frac{1}{m}}\) if \(\sigma\) is known, or
\(\frac{\bar{X}-\bar{Y}}\pm t_{m+n-2, \alpha/2} \cdot s_p
\sqrt{\frac{1}{n} + \frac{1}{m}}\) if \(\sigma\) is unknown.
</p>
</div>
</div>
<div id="outline-container-org835a760" class="outline-4">
<h4 id="org835a760"><span class="section-number-4">13.1.4</span> Unequal Variance</h4>
<div class="outline-text-4" id="text-13-1-4">
<p>
\(Z := \frac{\bar{X} - \bar{Y} - (\mu_X -
\mu_Y)}{{\sqrt{\frac{\sigma_X^2}{n} + \frac{\sigma_Y^2}{m}}}}\)
</p>

<p>
\(t := \frac{\bar{X} - \bar{Y} - (\mu_X -
\mu_Y)}{{\sqrt{\frac{s_X^2}{n} + \frac{s_Y^2}{m}}}}\), with \(df =
\frac{(a+b)^2}{\frac{a^2}{n-1} + \frac{b^2}{m-1}}\) where \(a =
\frac{s_X^2}{n}\) and \(b = \frac{s_Y^2}{m}\)
</p>
</div>
</div>
</div>
<div id="outline-container-org19286bc" class="outline-3">
<h3 id="org19286bc"><span class="section-number-3">13.2</span> Mann-Whitney Test</h3>
<div class="outline-text-3" id="text-13-2">
<p>
We take the smaller sample of size \(n_1\), and sum the ranks in that
sample. \(R' = n_1(m+n+1) -R\), and \(R* = min(R',R)\), we reject \(H_0: F
= G\) if \(R*\) is too small.
</p>

<p>
Test works for all distributions, and is robust to outliers.
</p>
</div>
</div>
<div id="outline-container-org6617a52" class="outline-3">
<h3 id="org6617a52"><span class="section-number-3">13.3</span> Paired Samples</h3>
<div class="outline-text-3" id="text-13-3">
<p>
\((X_i, Y_i)\) are paired and related to the same individual. \((X_i,
Y_i)\) is independent from \((X_j, Y_j)\). Compute \(D_i = Y_i - X_i\), To
test \(H_0 : \mu_D = d\), \(t = \frac{\bar{D} - \mu_D}{s_D/\sqrt{n}}\).
</p>

<p>
\(1-\alpha\) CI: \(\bar{D}\pm t_{n-1,\alpha/2}S_D/\sqrt{n}\)
</p>
</div>
</div>
<div id="outline-container-orgc61b900" class="outline-3">
<h3 id="orgc61b900"><span class="section-number-3">13.4</span> Ranked Test</h3>
<div class="outline-text-3" id="text-13-4">
<p>
\(W_+\) is the sum of ranks among all positive \(D_i\) and \(W_i\) is the
sum of ranks among all negative \(D_i\). We want to reject \(H_0\) if
\(W = min(W_+, W_-)\) is too large.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Created: 2017-12-02 Sat 21:54</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
